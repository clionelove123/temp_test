{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzTIiwyqzAtKr94/2L1mD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clionelove123/temp_test/blob/main/Chap_8_RNN_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from absl import app\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "f1r9WZU-PkN3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성하는 utility 함수를 정의합니다.\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "\n",
        "  return input_text, target_text\n",
        "\n",
        "# 학습에 필요한 설정값들을 지정합니다.\n",
        "data_dir = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')  # shakespeare\n",
        "#data_dir = './data/linux/input.txt'  # linux\n",
        "batch_size = 64      # Training : 64, Sampling : 1\n",
        "seq_length = 100     # Training : 100, Sampling : 1\n",
        "embedding_dim = 256  # Embedding 차원\n",
        "hidden_size = 1024   # 히든 레이어의 노드 개수\n",
        "num_epochs = 10\n",
        "\n",
        "# 학습에 사용할 txt 파일을 읽습니다.\n",
        "text = open(data_dir, 'rb').read().decode(encoding='utf-8')\n",
        "# 학습데이터에 포함된 모든 character들을 나타내는 변수인 vocab과\n",
        "# vocab에 id를 부여해 dict 형태로 만든 char2idx를 선언합니다.\n",
        "vocab = sorted(set(text))  # 유니크한 character 개수\n",
        "vocab_size = len(vocab)\n",
        "print('{} unique characters'.format(vocab_size))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# 학습 데이터를 character에서 integer로 변환합니다.\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# split_input_target 함수를 이용해서 input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성합니다.\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# tf.keras.Model을 이용해서 RNN 모델을 정의합니다.\n",
        "class RNN(tf.keras.Model):\n",
        " def __init__(self, batch_size):\n",
        "   super(RNN, self).__init__()\n",
        "   self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                    batch_input_shape=[batch_size, None])\n",
        "   self.hidden_layer_1 = tf.keras.layers.LSTM(hidden_size,\n",
        "                                             return_sequences=True,\n",
        "                                             stateful=True,\n",
        "                                             recurrent_initializer='glorot_uniform')\n",
        "   self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        " def call(self, x):\n",
        "   embedded_input = self.embedding_layer(x)\n",
        "   features = self.hidden_layer_1(embedded_input)\n",
        "   logits = self.output_layer(features)\n",
        "\n",
        "   return logits\n",
        "\n",
        "# sparse cross-entropy 손실 함수를 정의합니다.\n",
        "def sparse_cross_entropy_loss(labels, logits):\n",
        "  return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
        "\n",
        "# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 최적화를 위한 function을 정의합니다.\n",
        "@tf.function\n",
        "def train_step(model, input, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(input)\n",
        "    loss = sparse_cross_entropy_loss(target, logits)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다.\n",
        "\n",
        "  # start_sting을 integer 형태로 변환합니다.\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 샘플링 결과로 생성된 string을 저장할 배열을 초기화합니다.\n",
        "  text_generated = []\n",
        "\n",
        "  # 낮은 temperature 값은 더욱 정확한 텍스트를 생성합니다.\n",
        "  # 높은 temperature 값은 더욱 다양한 텍스트를 생성합니다.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기서 batch size = 1 입니다.\n",
        "  model.reset_states()\n",
        "  for i in range(num_sampling):\n",
        "    predictions = model(input_eval)\n",
        "    # 불필요한 batch dimension을 삭제합니다.\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # 모델의 예측결과에 기반해서 랜덤 샘플링을 하기위해 categorical distribution을 사용합니다.\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # 예측된 character를 다음 input으로 사용합니다.\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    # 샘플링 결과를 text_generated 배열에 추가합니다.\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCGO0mZfPhB_",
        "outputId": "e4674ecf-08b0-4f7a-8293-5aff11f73886"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Networks(RNN) 모델을 선언합니다.\n",
        "RNN_model = RNN(batch_size=batch_size)\n",
        "\n",
        "# 데이터 구조 파악을 위해서 예제로 임의의 하나의 배치 데이터 에측하고, 예측결과를 출력합니다.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = RNN_model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# 모델 정보를 출력합니다.\n",
        "RNN_model.summary()\n",
        "\n",
        "# checkpoint 데이터를 저장할 경로를 지정합니다.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start = time.time()\n",
        "\n",
        "  # 매 반복마다 hidden state를 초기화합니다. (최초의 hidden 값은 None입니다.)\n",
        "  hidden = RNN_model.reset_states()\n",
        "\n",
        "  for (batch_n, (input, target)) in enumerate(dataset):\n",
        "    loss = train_step(RNN_model, input, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 5회 반복마다 파라미터를 checkpoint로 저장합니다.\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "print(\"트레이닝이 끝났습니다!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esvnNKUfPl8T",
        "outputId": "96739db8-fa92-49d4-ab96-c1e0db45a6cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"rnn_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  16640     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 Batch 0 Loss 4.173638343811035\n",
            "Epoch 1 Batch 100 Loss 2.3512580394744873\n",
            "Epoch 1 Loss 2.0662\n",
            "Time taken for 1 epoch 17.527266263961792 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.109137535095215\n",
            "Epoch 2 Batch 100 Loss 1.8417805433273315\n",
            "Epoch 2 Loss 1.7136\n",
            "Time taken for 1 epoch 16.024904012680054 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6912083625793457\n",
            "Epoch 3 Batch 100 Loss 1.6249369382858276\n",
            "Epoch 3 Loss 1.5430\n",
            "Time taken for 1 epoch 14.869159698486328 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5398520231246948\n",
            "Epoch 4 Batch 100 Loss 1.540985107421875\n",
            "Epoch 4 Loss 1.4382\n",
            "Time taken for 1 epoch 14.42836594581604 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4039028882980347\n",
            "Epoch 5 Batch 100 Loss 1.4373939037322998\n",
            "Epoch 5 Loss 1.4197\n",
            "Time taken for 1 epoch 14.439326047897339 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3586089611053467\n",
            "Epoch 6 Batch 100 Loss 1.366432547569275\n",
            "Epoch 6 Loss 1.3651\n",
            "Time taken for 1 epoch 15.546181440353394 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2707356214523315\n",
            "Epoch 7 Batch 100 Loss 1.3225728273391724\n",
            "Epoch 7 Loss 1.2966\n",
            "Time taken for 1 epoch 14.687183618545532 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2577511072158813\n",
            "Epoch 8 Batch 100 Loss 1.3112913370132446\n",
            "Epoch 8 Loss 1.2612\n",
            "Time taken for 1 epoch 15.226078033447266 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2008066177368164\n",
            "Epoch 9 Batch 100 Loss 1.2036240100860596\n",
            "Epoch 9 Loss 1.2360\n",
            "Time taken for 1 epoch 14.70878005027771 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1259979009628296\n",
            "Epoch 10 Batch 100 Loss 1.1983892917633057\n",
            "Epoch 10 Loss 1.2170\n",
            "Time taken for 1 epoch 15.087146282196045 sec\n",
            "\n",
            "트레이닝이 끝났습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_RNN_model = RNN(batch_size=1)\n",
        "sampling_RNN_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "sampling_RNN_model.build(tf.TensorShape([1, None]))\n",
        "sampling_RNN_model.summary()\n",
        "\n",
        "# 샘플링을 시작합니다.\n",
        "print(\"샘플링을 시작합니다!\")\n",
        "print(generate_text(sampling_RNN_model, start_string=u' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYRWH64OQ2LG",
        "outputId": "98233f3b-1a59-48f3-c68c-289b47c24dbb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"rnn_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     multiple                  16640     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " dense_5 (Dense)             multiple                  66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "샘플링을 시작합니다!\n",
            " the face.\n",
            "I doth remain to this service everkife\n",
            "The subjucts my womenly hath to saliff swears. Waked nights that's we?\n",
            "\n",
            "ANTONIO:\n",
            "Good Clarence! let him sing and unshaped wares? We the people\n",
            "Is but a careful lany.\n",
            "\n",
            "ISABELLA:\n",
            "They have all presentlys an easy griming barreg Torder;\n",
            "And witness him so deep,\n",
            "Whereof what he hath seen a secret me?\n",
            "\n",
            "JULIET:\n",
            "I do not then, we will have believed the people strept.\n",
            "\n",
            "LUCENTIO:\n",
            "He is tof me, fear it;\n",
            "And now makes us known. If an head as me?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "I come to clasf: call here my Richmond?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Romeo, not tof your griefs: take no wetther? God for\n",
            "That I may lay as we stand for thee man;\n",
            "The freath of blood was so are full of news,\n",
            "That that ill-comfortable Paulina's brow,\n",
            "I, crail thee once twenty men alack; I am:\n",
            "Such silence slow to hear the noble wall?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Where thy nunter is the\n",
            "duke had recover his bory to weeping hood,\n",
            "Whet men by large than Romans is made onservess too:\n",
            "Those eyes may stay to fear the Tarkzous stovereast!\n",
            "\n",
            "CLORENCE:\n",
            "Now, Coriolanus lords, away.\n",
            "\n",
            "LADY CAPULET:\n",
            "Pompey, you were a king; by this day 'twas born.\n",
            "I like a soldier, how came it?\n",
            "\n",
            "First Murderer:\n",
            "Till What couch-dow, nature, yet among him;\n",
            "I know you know him! you command your worship, I cannot out thy sovereignty.\n",
            "\n",
            "VALUCI:\n",
            "O, were--\n",
            "\n",
            "BENVOLIO:\n",
            "Welcome, my last ceremonest:\n",
            "Yet hence wast Coling most within the one\n",
            "O dishonour for much admirous,\n",
            "Which we have broke to live in buty queen,\n",
            "And their comforted benefit did\n",
            "As that you does not too.\n",
            "\n",
            "CORIOLANUM:\n",
            "I know they learn me of the buttremover'd hate\n",
            "As keeping that puph a quiet where he has\n",
            "'Twould attom of reason as they waked; and he,\n",
            "And, from your brot of avoin wary weeping.\n",
            "\n",
            "MERCUTIO:\n",
            "But how lige this my bones the new-pose shand to accomplish that I.\n",
            "All this use on his jack?\n",
            "\n",
            "PORIXENES:\n",
            "O Thirst, d learn to silver in the crown,\n",
            "And but a soldier to his virtue?\n",
            "\n",
            "BENVOLIO:\n",
            "O perariliage, this is the name of is a face to behring\n",
            "but theerings muldiness. But, I do set not to\n",
            "stand on me, and need, or so?\n",
            "The more, who seems to be recover'd with child,\n",
            "When thine at our dafure will muse are mine?\n",
            "We did scure love; but stecle their flights\n",
            "In burne answered let me with thee with-she's a beggar,\n",
            "My father's soul within the midal of thy\n",
            "sight of thequell,\n",
            "Stamp, through with mine, and so we fight contrived,\n",
            "That thou should be this saked upon his sea\n",
            "One what's the share the knowlerge between banch of malidence, my life,\n",
            "That nevers come to die.\n",
            "\n",
            "ESCALUS:\n",
            "Ay, no! by mark away these course of both,\n",
            "With full disgract valours of his one aban.\n",
            "I know your bloody birth,--\n",
            "\n",
            "PETRUCHIO:\n",
            "This was the care, you know I am too plain.\n",
            "\n",
            "ERWARD:\n",
            "O, do.\n",
            "\n",
            "KING RICHARD II:\n",
            "This done too, out of nobe about to have.\n",
            "\n",
            "KATHARINA:\n",
            "Why, as he live, my lord!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "AITHARD III:\n",
            "A diding fellow is sun shalt be there to say:\n",
            "For I, as therefore but as you are this doth lace\n",
            "And fly enough to a thousand time ther services\n",
            "What I enough now; no, no more; no word'd arts,\n",
            "That, I prosper weaps, here's hove you merry, sir; marry, the Walk:\n",
            "How kill'd him, he cannot well go,\n",
            "Your watch not Gloucester's decree vis men.\n",
            "Now was the mortal followers forth father, you\n",
            "are not hot that an arr wend to virgy on their daughters to be sa\n",
            "Where's a troublous thine.\n",
            "\n",
            "MENENIUS:\n",
            "Nay, I'll go it; bury:\n",
            "The ce and bear you wroaght\n",
            "To use 't:\n",
            "Signior Peter Grumio's war; it is my friends. Your words\n",
            "Kine man, for movees of course, and my house,\n",
            "And love what heaven it lightly have we sit?\n",
            "\n",
            "SEBASTIAN:\n",
            "What it was not him; where 'tis done?\n",
            "A RICHARD III:\n",
            "Well have removed him, he lend him here,\n",
            "And take the new end thee that ever doth not pray us sit; or\n",
            "Word, in a fearAng forworth you amend,\n",
            "And best our colour hooour brace of stricts my harm\n",
            "By justice very friend conceiveder:\n",
            "Our treasuries, are thought\n",
            "The counselment have bellaw themse from you,\n",
            "Where he of your collacters, they to swow thy peace to say,\n",
            "Untim to care about the heavens.\n",
            "\n",
            "BAPTISTA:\n",
            "It my d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSVZs2TFPl-u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6kM8gybPmBT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UctG83KQPmDi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKIWNKk0PmGA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCOajMHxPmIJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2fsxSr3PmKq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2TCLlcjPmND"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkOEuDXrPmPN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_rhQ_pGPmRb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pg2og37fPmUG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3XpWtoMPmWo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qlaF6Y5PmY5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %run train_and_sampling.py"
      ],
      "metadata": {
        "id": "uQirp-BkLwpG"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}