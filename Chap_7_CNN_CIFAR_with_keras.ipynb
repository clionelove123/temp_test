{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB230cEWCOgElRkiUCF+oM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clionelove123/temp_test/blob/main/Chap_7_CNN_CIFAR_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Convolutional Neural Networks(CNN)을 이용한 MNIST 분류기(Classifier)\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9DEOwzum5PHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 데이터를 다운로드하고 데이터를 불러옵니다.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "# 이미지들을 float32 데이터 타입으로 변경합니다.\n",
        "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
        "# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다.\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환합니다.\n",
        "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10), axis=1)\n",
        "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10), axis=1)\n",
        "\n",
        "\n",
        "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot))\n",
        "train_data = train_data.repeat().shuffle(50000).batch(128)\n",
        "train_data_iter = iter(train_data)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test_one_hot))\n",
        "test_data = test_data.batch(1000)\n",
        "test_data_iter = iter(test_data)"
      ],
      "metadata": {
        "id": "iEootGt15SQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.Model을 이용해서 CNN 모델을 정의합니다.\n",
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # 첫번째 convolutional layer - 하나의 RGB 이미지를 64개의 특징들(feature)으로 맵핑(mapping)합니다.\n",
        "    self.conv_layer_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "    self.pool_layer_1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2)\n",
        "    # 두번째 convolutional layer - 64개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(mapping)합니다.\n",
        "    self.conv_layer_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "    self.pool_layer_2 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2)\n",
        "    # 세번째 convolutional layer\n",
        "    self.conv_layer_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "    # 네번째 convolutional layer\n",
        "    self.conv_layer_4 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "    # 다섯번째 convolutional layer\n",
        "    self.conv_layer_5 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "    # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
        "    # 이를 384개의 특징들로 맵핑(mapping)합니다.\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.fc_layer_1 = tf.keras.layers.Dense(384, activation='relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "    # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(mapsping)합니다.\n",
        "    self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
        "\n",
        "  def call(self, x, is_training):\n",
        "    # 입력 이미지\n",
        "    h_conv1 = self.conv_layer_1(x)\n",
        "    h_pool1 = self.pool_layer_1(h_conv1)\n",
        "    h_conv2 = self.conv_layer_2(h_pool1)\n",
        "    h_pool2 = self.pool_layer_2(h_conv2)\n",
        "    h_conv3 = self.conv_layer_3(h_pool2)\n",
        "    h_conv4 = self.conv_layer_4(h_conv3)\n",
        "    h_conv5 = self.conv_layer_5(h_conv4)\n",
        "    h_conv5_flat = self.flatten_layer(h_conv5)\n",
        "    h_fc1 = self.fc_layer_1(h_conv5_flat)\n",
        "    # Dropout - 모델의 복잡도를 컨트롤합니다. 특징들의 co-adaptation을 방지합니다.\n",
        "    h_fc1_drop = self.dropout(h_fc1, training=is_training)\n",
        "    logits = self.output_layer(h_fc1_drop)\n",
        "    y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "    return y_pred, logits\n"
      ],
      "metadata": {
        "id": "IBraqtF6DTsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-entropy 손실 함수를 정의합니다.\n",
        "@tf.function\n",
        "def cross_entropy_loss(logits, y):\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n",
        "optimizer = tf.optimizers.Adam(1e-4)\n",
        "\n",
        "# 모델의 정확도를 출력하는 함수를 정의합니다.\n",
        "@tf.function\n",
        "def compute_accuracy(y_pred, y):\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "obixAvDz5YeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화를 위한 function을 정의합니다.\n",
        "@tf.function\n",
        "def train_step(model, x, y, is_training):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred, logits = model(x, is_training)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "Ldq2cpG_Acbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Networks(CNN) 모델을 선언합니다.\n",
        "CNN_model = CNN()\n",
        "\n",
        "# 20000 Step만큼 최적화를 수행합니다.\n",
        "for i in range(30000):\n",
        "  batch_x, batch_y = next(train_data_iter)\n",
        "\n",
        "  # 100 Step마다 training 데이터셋에 대한 정확도와 loss를 출력합니다.\n",
        "  if i % 500 == 0:\n",
        "    train_accuracy = compute_accuracy(CNN_model(batch_x, False)[0], batch_y)\n",
        "    loss_print = cross_entropy_loss(CNN_model(batch_x, False)[1], batch_y)\n",
        "    test_accuracy = 0.0\n",
        "    test_loss = 0.0\n",
        "    test_data_iter = iter(test_data)\n",
        "    for j in range(10):\n",
        "      test_batch_x, test_batch_y = next(test_data_iter)\n",
        "      test_accuracy = test_accuracy + compute_accuracy(CNN_model(test_batch_x, False)[0], test_batch_y).numpy()\n",
        "      test_loss = test_loss + cross_entropy_loss(CNN_model(test_batch_x, False)[1], test_batch_y).numpy()\n",
        "    test_accuracy = test_accuracy / 10\n",
        "    test_loss = test_loss / 10\n",
        "    print(\"반복(Epoch): %d, 트레이닝 데이터 정확도: %f, 손실: %f 테스트 데이터 정확도: %f 테스트 데이터 손실실: %f\" % (i, train_accuracy, loss_print,test_accuracy,test_loss))\n",
        "  # 20% 확률의 Dropout을 이용해서 학습을 진행합니다.\n",
        "  train_step(CNN_model, batch_x, batch_y, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63XWHLZ05MN0",
        "outputId": "e003942f-bbf3-44cd-f1a4-500828dc557c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "반복(Epoch): 0, 트레이닝 데이터 정확도: 0.062500, 손실: 2.303861 테스트 데이터 정확도: 0.093500 테스트 데이터 손실실: 2.303268\n",
            "반복(Epoch): 500, 트레이닝 데이터 정확도: 0.515625, 손실: 1.433643 테스트 데이터 정확도: 0.502900 테스트 데이터 손실실: 1.388802\n",
            "반복(Epoch): 1000, 트레이닝 데이터 정확도: 0.601562, 손실: 1.227224 테스트 데이터 정확도: 0.566800 테스트 데이터 손실실: 1.204508\n",
            "반복(Epoch): 1500, 트레이닝 데이터 정확도: 0.695312, 손실: 0.965826 테스트 데이터 정확도: 0.612600 테스트 데이터 손실실: 1.089317\n",
            "반복(Epoch): 2000, 트레이닝 데이터 정확도: 0.640625, 손실: 1.048496 테스트 데이터 정확도: 0.626500 테스트 데이터 손실실: 1.042707\n",
            "반복(Epoch): 2500, 트레이닝 데이터 정확도: 0.640625, 손실: 1.049147 테스트 데이터 정확도: 0.653100 테스트 데이터 손실실: 0.981754\n",
            "반복(Epoch): 3000, 트레이닝 데이터 정확도: 0.781250, 손실: 0.701672 테스트 데이터 정확도: 0.686400 테스트 데이터 손실실: 0.893082\n",
            "반복(Epoch): 3500, 트레이닝 데이터 정확도: 0.664062, 손실: 0.910271 테스트 데이터 정확도: 0.696300 테스트 데이터 손실실: 0.873902\n",
            "반복(Epoch): 4000, 트레이닝 데이터 정확도: 0.718750, 손실: 0.824358 테스트 데이터 정확도: 0.708400 테스트 데이터 손실실: 0.839453\n",
            "반복(Epoch): 4500, 트레이닝 데이터 정확도: 0.710938, 손실: 0.674030 테스트 데이터 정확도: 0.722700 테스트 데이터 손실실: 0.794805\n",
            "반복(Epoch): 5000, 트레이닝 데이터 정확도: 0.804688, 손실: 0.541786 테스트 데이터 정확도: 0.731600 테스트 데이터 손실실: 0.772697\n",
            "반복(Epoch): 5500, 트레이닝 데이터 정확도: 0.789062, 손실: 0.531088 테스트 데이터 정확도: 0.740900 테스트 데이터 손실실: 0.757666\n",
            "반복(Epoch): 6000, 트레이닝 데이터 정확도: 0.734375, 손실: 0.782268 테스트 데이터 정확도: 0.729800 테스트 데이터 손실실: 0.790539\n",
            "반복(Epoch): 6500, 트레이닝 데이터 정확도: 0.804688, 손실: 0.598974 테스트 데이터 정확도: 0.742100 테스트 데이터 손실실: 0.750167\n",
            "반복(Epoch): 7000, 트레이닝 데이터 정확도: 0.828125, 손실: 0.478940 테스트 데이터 정확도: 0.745800 테스트 데이터 손실실: 0.743231\n",
            "반복(Epoch): 7500, 트레이닝 데이터 정확도: 0.890625, 손실: 0.401873 테스트 데이터 정확도: 0.752000 테스트 데이터 손실실: 0.728195\n",
            "반복(Epoch): 8000, 트레이닝 데이터 정확도: 0.796875, 손실: 0.548736 테스트 데이터 정확도: 0.751100 테스트 데이터 손실실: 0.736506\n",
            "반복(Epoch): 8500, 트레이닝 데이터 정확도: 0.898438, 손실: 0.298261 테스트 데이터 정확도: 0.759400 테스트 데이터 손실실: 0.716007\n",
            "반복(Epoch): 9000, 트레이닝 데이터 정확도: 0.890625, 손실: 0.319177 테스트 데이터 정확도: 0.748200 테스트 데이터 손실실: 0.761515\n",
            "반복(Epoch): 9500, 트레이닝 데이터 정확도: 0.906250, 손실: 0.272066 테스트 데이터 정확도: 0.762600 테스트 데이터 손실실: 0.718543\n",
            "반복(Epoch): 10000, 트레이닝 데이터 정확도: 0.867188, 손실: 0.384839 테스트 데이터 정확도: 0.761200 테스트 데이터 손실실: 0.758498\n",
            "반복(Epoch): 10500, 트레이닝 데이터 정확도: 0.890625, 손실: 0.225315 테스트 데이터 정확도: 0.767300 테스트 데이터 손실실: 0.743698\n",
            "반복(Epoch): 11000, 트레이닝 데이터 정확도: 0.929688, 손실: 0.254159 테스트 데이터 정확도: 0.761000 테스트 데이터 손실실: 0.796594\n",
            "반복(Epoch): 11500, 트레이닝 데이터 정확도: 0.929688, 손실: 0.185974 테스트 데이터 정확도: 0.761800 테스트 데이터 손실실: 0.806148\n",
            "반복(Epoch): 12000, 트레이닝 데이터 정확도: 0.953125, 손실: 0.211090 테스트 데이터 정확도: 0.762500 테스트 데이터 손실실: 0.831179\n",
            "반복(Epoch): 12500, 트레이닝 데이터 정확도: 0.953125, 손실: 0.178790 테스트 데이터 정확도: 0.765000 테스트 데이터 손실실: 0.855682\n",
            "반복(Epoch): 13000, 트레이닝 데이터 정확도: 0.960938, 손실: 0.140719 테스트 데이터 정확도: 0.767500 테스트 데이터 손실실: 0.837975\n",
            "반복(Epoch): 13500, 트레이닝 데이터 정확도: 0.984375, 손실: 0.102837 테스트 데이터 정확도: 0.756700 테스트 데이터 손실실: 0.902685\n",
            "반복(Epoch): 14000, 트레이닝 데이터 정확도: 0.984375, 손실: 0.062954 테스트 데이터 정확도: 0.769500 테스트 데이터 손실실: 0.885556\n",
            "반복(Epoch): 14500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.058870 테스트 데이터 정확도: 0.763800 테스트 데이터 손실실: 0.929611\n",
            "반복(Epoch): 15000, 트레이닝 데이터 정확도: 0.976562, 손실: 0.076791 테스트 데이터 정확도: 0.766000 테스트 데이터 손실실: 0.961741\n",
            "반복(Epoch): 15500, 트레이닝 데이터 정확도: 0.992188, 손실: 0.048868 테스트 데이터 정확도: 0.765400 테스트 데이터 손실실: 1.030222\n",
            "반복(Epoch): 16000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.021377 테스트 데이터 정확도: 0.763500 테스트 데이터 손실실: 1.051068\n",
            "반복(Epoch): 16500, 트레이닝 데이터 정확도: 0.984375, 손실: 0.059786 테스트 데이터 정확도: 0.770900 테스트 데이터 손실실: 1.002684\n",
            "반복(Epoch): 17000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.028525 테스트 데이터 정확도: 0.769700 테스트 데이터 손실실: 1.059506\n",
            "반복(Epoch): 17500, 트레이닝 데이터 정확도: 0.992188, 손실: 0.031360 테스트 데이터 정확도: 0.766000 테스트 데이터 손실실: 1.079919\n",
            "반복(Epoch): 18000, 트레이닝 데이터 정확도: 0.992188, 손실: 0.019783 테스트 데이터 정확도: 0.767500 테스트 데이터 손실실: 1.114050\n",
            "반복(Epoch): 18500, 트레이닝 데이터 정확도: 0.984375, 손실: 0.034332 테스트 데이터 정확도: 0.767400 테스트 데이터 손실실: 1.125303\n",
            "반복(Epoch): 19000, 트레이닝 데이터 정확도: 0.992188, 손실: 0.031800 테스트 데이터 정확도: 0.756900 테스트 데이터 손실실: 1.167661\n",
            "반복(Epoch): 19500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.014694 테스트 데이터 정확도: 0.764900 테스트 데이터 손실실: 1.231479\n",
            "반복(Epoch): 20000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.010245 테스트 데이터 정확도: 0.769100 테스트 데이터 손실실: 1.206072\n",
            "반복(Epoch): 20500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.013488 테스트 데이터 정확도: 0.770500 테스트 데이터 손실실: 1.267812\n",
            "반복(Epoch): 21000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.009050 테스트 데이터 정확도: 0.765200 테스트 데이터 손실실: 1.220213\n",
            "반복(Epoch): 21500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.031098 테스트 데이터 정확도: 0.758700 테스트 데이터 손실실: 1.248733\n",
            "반복(Epoch): 22000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.005934 테스트 데이터 정확도: 0.770100 테스트 데이터 손실실: 1.239580\n",
            "반복(Epoch): 22500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.011907 테스트 데이터 정확도: 0.772400 테스트 데이터 손실실: 1.254481\n",
            "반복(Epoch): 23000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.007373 테스트 데이터 정확도: 0.767400 테스트 데이터 손실실: 1.288901\n",
            "반복(Epoch): 23500, 트레이닝 데이터 정확도: 0.984375, 손실: 0.022704 테스트 데이터 정확도: 0.768600 테스트 데이터 손실실: 1.314846\n",
            "반복(Epoch): 24000, 트레이닝 데이터 정확도: 0.992188, 손실: 0.030667 테스트 데이터 정확도: 0.763000 테스트 데이터 손실실: 1.349884\n",
            "반복(Epoch): 24500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.013063 테스트 데이터 정확도: 0.769200 테스트 데이터 손실실: 1.331637\n",
            "반복(Epoch): 25000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.008932 테스트 데이터 정확도: 0.771200 테스트 데이터 손실실: 1.296357\n",
            "반복(Epoch): 25500, 트레이닝 데이터 정확도: 0.992188, 손실: 0.019946 테스트 데이터 정확도: 0.768700 테스트 데이터 손실실: 1.362961\n",
            "반복(Epoch): 26000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.009461 테스트 데이터 정확도: 0.767400 테스트 데이터 손실실: 1.380985\n",
            "반복(Epoch): 26500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.004264 테스트 데이터 정확도: 0.771000 테스트 데이터 손실실: 1.386615\n",
            "반복(Epoch): 27000, 트레이닝 데이터 정확도: 0.984375, 손실: 0.025236 테스트 데이터 정확도: 0.771300 테스트 데이터 손실실: 1.353109\n",
            "반복(Epoch): 27500, 트레이닝 데이터 정확도: 0.992188, 손실: 0.024834 테스트 데이터 정확도: 0.763600 테스트 데이터 손실실: 1.418465\n",
            "반복(Epoch): 28000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.007574 테스트 데이터 정확도: 0.763700 테스트 데이터 손실실: 1.425097\n",
            "반복(Epoch): 28500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.005374 테스트 데이터 정확도: 0.769000 테스트 데이터 손실실: 1.390059\n",
            "반복(Epoch): 29000, 트레이닝 데이터 정확도: 1.000000, 손실: 0.002321 테스트 데이터 정확도: 0.771300 테스트 데이터 손실실: 1.439291\n",
            "반복(Epoch): 29500, 트레이닝 데이터 정확도: 1.000000, 손실: 0.004037 테스트 데이터 정확도: 0.766600 테스트 데이터 손실실: 1.465074\n"
          ]
        }
      ]
    }
  ]
}