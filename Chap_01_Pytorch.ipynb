{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNv6arR/ZtL4vkUVG04/jYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clionelove123/temp_test/blob/main/Chap_01_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2FGfFSH8F0D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "_jWKdPlR8QSz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "iMFiA_sC8QdQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVprwCfp8TZ4",
        "outputId": "29a217ba-df2b-478a-e996-476a8fbae757"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.5557, 0.3196],\n",
            "        [0.4799, 0.2007]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92wlbI88Tce",
        "outputId": "e9e454aa-4035-4226-96ce-de8fd4c4c496"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.3620, 0.9448, 0.9060],\n",
            "        [0.8113, 0.8432, 0.0383]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBzmJisj8Te-",
        "outputId": "91b85fb2-f61c-4119-c256-6ff7ebbda1dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU가 존재하면 텐서를 이동합니다\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "uL2FU_oq8ThX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejHCLnQ58Tjr",
        "outputId": "4e1d7d1e-08ad-4b5e-9951-10b5dadfc33f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvcLiDJY8Tl-",
        "outputId": "66824275-7564-4a71-cde9-2f2453dbe491"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n",
        "# ``tensor.T`` 는 텐서의 전치(transpose)를 반환합니다.\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "print(y1)\n",
        "print(y2)\n",
        "print(y3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gggHDVB8auF",
        "outputId": "9c53b75e-3d55-464c-a133-13483d04a89a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n",
            "tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n",
            "tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "\n",
        "print(z1)\n",
        "print(z2)\n",
        "print(z3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvacZs7_HXz8",
        "outputId": "c906e0af-dbbc-45e4-f013-0543737d5f5e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Dictionary\n",
        "dict_1 = {\n",
        "    \"python\": 0,\n",
        "    \"java\":1,\n",
        "    \"c++\":2,\n",
        "    \"ruby\":3,\n",
        "    \"html\":4,\n",
        "    \"key\":\"value\",\n",
        "}\n",
        "\n",
        "print(dict_1)\n",
        "print(dict_1[\"c++\"])\n",
        "\n",
        "del dict_1[\"key\"]\n",
        "print(dict_1)\n",
        "\n",
        "dict_1[\"pytorch\"]=5\n",
        "print(dict_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS5yiJlH_Jrp",
        "outputId": "092e31b6-eccf-4cb9-8fde-d06ade85988c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'python': 0, 'java': 1, 'c++': 2, 'ruby': 3, 'html': 4, 'key': 'value'}\n",
            "2\n",
            "{'python': 0, 'java': 1, 'c++': 2, 'ruby': 3, 'html': 4}\n",
            "{'python': 0, 'java': 1, 'c++': 2, 'ruby': 3, 'html': 4, 'pytorch': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Loop\n",
        "\n",
        "list_1 = [1,2,3]\n",
        "\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "\n",
        "print(\"\\n\")\n",
        "for i in range(0,-5,-1):\n",
        "    print(i)\n",
        "\n",
        "\n",
        "print(list_1)\n",
        "for i in list_1:\n",
        "    print(i)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(dict_1)\n",
        "for key in dict_1:\n",
        "    print(key,dict_1[key])\n",
        "\n",
        "a=0\n",
        "while(a<10):\n",
        "    print(a)\n",
        "    a+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVk6c1Y7_H4I",
        "outputId": "3da0218f-571a-43cc-bc52-96f9017a6a27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "\n",
            "\n",
            "0\n",
            "-1\n",
            "-2\n",
            "-3\n",
            "-4\n",
            "[1, 2, 3]\n",
            "1\n",
            "2\n",
            "3\n",
            "\n",
            "\n",
            "{'python': 0, 'java': 1, 'c++': 2, 'ruby': 3, 'html': 4, 'pytorch': 5}\n",
            "python 0\n",
            "java 1\n",
            "c++ 2\n",
            "ruby 3\n",
            "html 4\n",
            "pytorch 5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Condition\n",
        "\n",
        "print(2==2)\n",
        "print(2==3)\n",
        "print(2!=2)\n",
        "print(2!=3)\n",
        "\n",
        "print(dict_1)\n",
        "for key in dict_1:\n",
        "    if key==\"ruby\":\n",
        "        print(dict_1[key])\n",
        "    else:\n",
        "        print(\"not ruby\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrdN19iy_AlN",
        "outputId": "54d9727d-3cba-4cf4-94a8-72ae0373f8e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "True\n",
            "{'python': 0, 'java': 1, 'c++': 2, 'ruby': 3, 'html': 4, 'pytorch': 5}\n",
            "not ruby\n",
            "not ruby\n",
            "not ruby\n",
            "3\n",
            "not ruby\n",
            "not ruby\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Function\n",
        "def factorial(num):\n",
        "    result = 1\n",
        "    for i in range(1,num+1):\n",
        "        result = result*i\n",
        "    return result\n",
        "\n",
        "print(factorial(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmnjugF8_Ddr",
        "outputId": "ec8de1e0-eae0-4a85-d543-377f62e827e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Class and Objects\n",
        "class Jedi(object):\n",
        "    def __init__(self,name,droid):\n",
        "        self.name = name\n",
        "        self.droid = droid\n",
        "        self.exp = 0\n",
        "        \n",
        "    def train(self):\n",
        "        self.exp += 10\n",
        "\n",
        "anakin = Jedi(\"Anakin Skywalker\",\"R2D2\")\n",
        "anakin.train()\n",
        "\n",
        "print(anakin.name)\n",
        "print(anakin.droid)\n",
        "print(anakin.exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfZsR4tQ_E62",
        "outputId": "abf2de31-06d0-4ff2-f9b2-f502902b0702"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anakin Skywalker\n",
            "R2D2\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 불러오기\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "C5SlxvDX8hcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b07a23b-d04c-4a1b-b266-5ed7d19b8263"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 14957390.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 269692.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5024550.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 17287651.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 시각화하기\n",
        "\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "ZjHnE4rS8he9",
        "outputId": "5d4cf60b-6361-4cc9-acf8-5ab2f4c2f70d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일에서 사용자 정의 데이터셋 만들기\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "#__init__\n",
        "#__init__ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. 여기서는 이미지와 주석 파일(annotation_file)이 포함된 디렉토리와 (다음 장에서 자세히 살펴볼) 두가지 변형(transform)을 초기화합니다.\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "#__len__\n",
        "#__len__ 함수는 데이터셋의 샘플 개수를 반환합니다.\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "#__getitem__\n",
        "#__getitem__ 함수는 주어진 인덱스 idx 에 해당하는 샘플을 데이터셋에서 불러오고 반환합니다. 인덱스를 기반으로, 디스크에서 이미지의 위치를 식별하고, read_image 를 사용하여 이미지를 텐서로 변환하고, self.img_labels 의 csv 데이터로부터 해당하는 정답(label)을 가져오고, (해당하는 경우) 변형(transform) 함수들을 호출한 뒤, 텐서 이미지와 라벨을 Python 사전(dict)형으로 반환합니다.\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "7j3qZrqr8hhh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader로 학습용 데이터 준비하기\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "ZqZ2I7No8awx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader를 통해 순회하기(iterate)\n",
        "# 이미지와 정답(label)을 표시합니다.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "AWvlNYKA8azL",
        "outputId": "1a491909-2acc-4463-809c-8a97240d5515"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TORCH.AUTOGRAD를 사용한 자동 미분하기\n",
        "#신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파입니다. 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 변화도(gradient)에 따라 조정됩니다.\n",
        "#이러한 변화도를 계산하기 위해 PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원합니다.\n",
        "#입력 x, 매개변수 w와 b , 그리고 일부 손실 함수가 있는 가장 간단한 단일 계층 신경망을 가정하겠습니다. PyTorch에서는 다음과 같이 정의할 수 있습니다:\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "muLlUvMg84bK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연산 그래프를 구성하기 위해 텐서에 적용하는 함수는 사실 Function 클래스의 객체입니다. \n",
        "# 이 객체는 순전파 방향으로 함수를 계산하는 방법과, 역방향 전파 단계에서 도함수(derivative)를 계산하는 방법을 알고 있습니다. \n",
        "# 역방향 전파 함수에 대한 참조(reference)는 텐서의 grad_fn 속성에 저장됩니다\n",
        "\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPtKhOep84d0",
        "outputId": "ecabdf60-08c2-4450-881e-92c0b7db3d92"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7fa251c749a0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fa251b4f430>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 변화도(Gradient) 계산하기\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USKp98wL84gP",
        "outputId": "86a1a95a-300f-4666-fa3d-d12bd4266ce9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0983, 0.1227, 0.0493],\n",
            "        [0.0983, 0.1227, 0.0493],\n",
            "        [0.0983, 0.1227, 0.0493],\n",
            "        [0.0983, 0.1227, 0.0493],\n",
            "        [0.0983, 0.1227, 0.0493]])\n",
            "tensor([0.0983, 0.1227, 0.0493])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 변화도 추적 멈추기\n",
        "# 기본적으로, requires_grad=True인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원합니다. \n",
        "# 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산만 필요한 경우에는, 이러한 추적이나 지원이 필요 없을 수 있습니다. \n",
        "# 연산 코드를 torch.no_grad() 블록으로 둘러싸서 연산 추적을 멈출 수 있습니다:\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrufLu27AP_M",
        "outputId": "c88c3d32-ab3d-4c10-9739-ba1a68b40435"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 구성하기\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "P4dEfGWF-Kao"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 위한 장치 얻기\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkLHRtoB-Kdn",
        "outputId": "780ec7c1-7b4a-442c-9722-51ce4078c6b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 정의하기\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "D6Is8tuK-Kf2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iIKf3eh-Kh_",
        "outputId": "ab5b8bd0-1416-4890-ee68-ac4638bfeebd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4F1CnxM-Kkk",
        "outputId": "da767fc2-8b35-43ec-eb65-65aad1ff25cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 계층(Layer)\n",
        "# 1) Input layer\n",
        "\n",
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unEaokJW-P3H",
        "outputId": "c1fdf33e-0eb4-4666-92fe-0e9524ad73c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Flatten Layer: 계층을 초기화하여 각 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환합니다. \n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsnLiDRx-Kmh",
        "outputId": "9df5e423-7b64-475f-ad46-c4d3b4254ab4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Linear Layer: 저장된 가중치(weight)와 편향(bias)을 사용하여 입력에 선형 변환(linear transformation)을 적용하는 모듈입니다.\n",
        "\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p25YVmcx-Kox",
        "outputId": "35f8a38b-8a50-4010-9d26-b8bfb2c5dbc2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Relu Layer (Activation Function): 비선형 활성화(activation)는 모델의 입력과 출력 사이에 복잡한 관계(mapping)를 만듭니다. 비선형 활성화는 선형 변환 후에 적용되어 비선형성(nonlinearity) 을 도입하고, 신경망이 다양한 현상을 학습할 수 있도록 돕습니다.\n",
        "\n",
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJbwnXWG-KrX",
        "outputId": "7c3df1fe-7ed6-4b50-8920-b40d19a6b6cc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.2131,  0.2024, -0.2251, -0.0928,  0.2118,  0.1111,  0.2854, -0.1693,\n",
            "          0.3290,  0.3154,  0.5193, -0.0149, -0.4104, -0.3148, -0.0054,  0.1865,\n",
            "         -0.6926,  0.2264,  0.2290, -0.2724],\n",
            "        [ 0.3068, -0.1178, -0.1923, -0.0824,  0.0717,  0.1687, -0.2189, -0.0738,\n",
            "         -0.0552,  0.2179,  0.1484,  0.2341, -0.1846, -0.1598,  0.1332, -0.0593,\n",
            "         -0.8149,  0.1257, -0.1410,  0.0384],\n",
            "        [-0.0433, -0.2785, -0.4965,  0.1362,  0.3524, -0.0227, -0.0545, -0.4347,\n",
            "          0.2922,  0.2575,  0.6161, -0.1417, -0.1835, -0.5301,  0.0966,  0.1929,\n",
            "         -0.8846,  0.4627,  0.2923,  0.0266]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.2131, 0.2024, 0.0000, 0.0000, 0.2118, 0.1111, 0.2854, 0.0000, 0.3290,\n",
            "         0.3154, 0.5193, 0.0000, 0.0000, 0.0000, 0.0000, 0.1865, 0.0000, 0.2264,\n",
            "         0.2290, 0.0000],\n",
            "        [0.3068, 0.0000, 0.0000, 0.0000, 0.0717, 0.1687, 0.0000, 0.0000, 0.0000,\n",
            "         0.2179, 0.1484, 0.2341, 0.0000, 0.0000, 0.1332, 0.0000, 0.0000, 0.1257,\n",
            "         0.0000, 0.0384],\n",
            "        [0.0000, 0.0000, 0.0000, 0.1362, 0.3524, 0.0000, 0.0000, 0.0000, 0.2922,\n",
            "         0.2575, 0.6161, 0.0000, 0.0000, 0.0000, 0.0966, 0.1929, 0.0000, 0.4627,\n",
            "         0.2923, 0.0266]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential\" 순서를 갖는 모듈의 컨테이너입니다. 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달됩니다\n",
        "\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "metadata": {
        "id": "i698tKYr-Ktz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax: 신경망의 마지막 선형 계층은 nn.Softmax 모듈에 전달될 ([-\\infty, \\infty] 범위의 원시 값(raw value)인) logits 를 반환합니다. \n",
        "# logits는 모델의 각 분류(class)에 대한 예측 확률을 나타내도록 [0, 1] 범위로 비례하여 조정(scale)됩니다. dim 매개변수는 값의 합이 1이 되는 차원을 나타냅니다.\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "metadata": {
        "id": "kCY2Twcj-W1_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 매개변수\n",
        "# 신경망 내부의 많은 계층들은 매개변수화(parameterize) 됩니다. 즉, 학습 중에 최적화되는 가중치와 편향과 연관지어집니다. nn.Module 을 상속하면 모델 객체 내부의 모든 필드들이 자동으로 추적(track)되며, 모델의 parameters() 및 named_parameters() 메소드로 모든 매개변수에 접근할 수 있게 됩니다.\n",
        "# 이 예제에서는 각 매개변수들을 순회하며(iterate), 매개변수의 크기와 값을 출력합니다.\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIBW4ut7-W45",
        "outputId": "a50b8ada-e621-4d27-a974-db5903a55317"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0220,  0.0056,  0.0253,  ...,  0.0235,  0.0162,  0.0292],\n",
            "        [ 0.0182,  0.0282, -0.0299,  ..., -0.0214, -0.0069, -0.0097]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0107, -0.0116], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0048, -0.0129,  0.0278,  ..., -0.0223, -0.0365,  0.0275],\n",
            "        [ 0.0441,  0.0097, -0.0308,  ...,  0.0108,  0.0433,  0.0424]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0292, -0.0043], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0228,  0.0010,  0.0315,  ..., -0.0271,  0.0431, -0.0034],\n",
            "        [-0.0235, -0.0317, -0.0291,  ..., -0.0401,  0.0224,  0.0109]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0242,  0.0359], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 매개변수 최적화하기\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "T86F6Nmp84iq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터(Hyperparameter)\n",
        "# 하이퍼파라미터(Hyperparameter)는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수입니다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있습니다. (하이퍼파라미터 튜닝(tuning)에 대해 더 알아보기)\n",
        "# 학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다:\n",
        "# 에폭(epoch) 수 - 데이터셋을 반복하는 횟수\n",
        "# 배치 크기(batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
        "# 학습률(learning rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다.\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "YW61gg1o9XiF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수를 초기화합니다.\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "CmpLaFtC9Xkh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "UaatuGI09Xm9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # 예측(prediction)과 손실(loss) 계산\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "AMXR_S-n9XpG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s3sBM1A9XsG",
        "outputId": "43746895-c139-405b-e7ea-d113050901a9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304576  [   64/60000]\n",
            "loss: 2.295013  [ 6464/60000]\n",
            "loss: 2.279346  [12864/60000]\n",
            "loss: 2.277350  [19264/60000]\n",
            "loss: 2.252198  [25664/60000]\n",
            "loss: 2.221682  [32064/60000]\n",
            "loss: 2.236434  [38464/60000]\n",
            "loss: 2.197822  [44864/60000]\n",
            "loss: 2.201363  [51264/60000]\n",
            "loss: 2.165012  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 2.161640 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.167652  [   64/60000]\n",
            "loss: 2.167600  [ 6464/60000]\n",
            "loss: 2.111714  [12864/60000]\n",
            "loss: 2.133813  [19264/60000]\n",
            "loss: 2.077821  [25664/60000]\n",
            "loss: 2.008983  [32064/60000]\n",
            "loss: 2.050390  [38464/60000]\n",
            "loss: 1.962454  [44864/60000]\n",
            "loss: 1.980718  [51264/60000]\n",
            "loss: 1.900085  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.6%, Avg loss: 1.901892 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.927680  [   64/60000]\n",
            "loss: 1.910032  [ 6464/60000]\n",
            "loss: 1.794750  [12864/60000]\n",
            "loss: 1.846205  [19264/60000]\n",
            "loss: 1.723565  [25664/60000]\n",
            "loss: 1.662939  [32064/60000]\n",
            "loss: 1.700228  [38464/60000]\n",
            "loss: 1.581804  [44864/60000]\n",
            "loss: 1.622482  [51264/60000]\n",
            "loss: 1.515019  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.3%, Avg loss: 1.529654 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.586040  [   64/60000]\n",
            "loss: 1.561086  [ 6464/60000]\n",
            "loss: 1.412321  [12864/60000]\n",
            "loss: 1.496987  [19264/60000]\n",
            "loss: 1.367015  [25664/60000]\n",
            "loss: 1.354981  [32064/60000]\n",
            "loss: 1.375484  [38464/60000]\n",
            "loss: 1.280122  [44864/60000]\n",
            "loss: 1.330204  [51264/60000]\n",
            "loss: 1.235281  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.255348 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.321990  [   64/60000]\n",
            "loss: 1.313825  [ 6464/60000]\n",
            "loss: 1.152331  [12864/60000]\n",
            "loss: 1.268744  [19264/60000]\n",
            "loss: 1.136256  [25664/60000]\n",
            "loss: 1.158491  [32064/60000]\n",
            "loss: 1.176630  [38464/60000]\n",
            "loss: 1.097072  [44864/60000]\n",
            "loss: 1.148937  [51264/60000]\n",
            "loss: 1.074867  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.2%, Avg loss: 1.088124 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.148578  [   64/60000]\n",
            "loss: 1.161188  [ 6464/60000]\n",
            "loss: 0.986163  [12864/60000]\n",
            "loss: 1.129102  [19264/60000]\n",
            "loss: 0.993113  [25664/60000]\n",
            "loss: 1.027788  [32064/60000]\n",
            "loss: 1.054017  [38464/60000]\n",
            "loss: 0.982480  [44864/60000]\n",
            "loss: 1.031234  [51264/60000]\n",
            "loss: 0.975545  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.981293 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.029595  [   64/60000]\n",
            "loss: 1.063541  [ 6464/60000]\n",
            "loss: 0.874200  [12864/60000]\n",
            "loss: 1.036527  [19264/60000]\n",
            "loss: 0.901841  [25664/60000]\n",
            "loss: 0.936327  [32064/60000]\n",
            "loss: 0.973981  [38464/60000]\n",
            "loss: 0.909385  [44864/60000]\n",
            "loss: 0.949981  [51264/60000]\n",
            "loss: 0.909693  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.0%, Avg loss: 0.909108 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.943211  [   64/60000]\n",
            "loss: 0.996833  [ 6464/60000]\n",
            "loss: 0.795067  [12864/60000]\n",
            "loss: 0.971283  [19264/60000]\n",
            "loss: 0.840371  [25664/60000]\n",
            "loss: 0.869526  [32064/60000]\n",
            "loss: 0.917849  [38464/60000]\n",
            "loss: 0.861694  [44864/60000]\n",
            "loss: 0.891667  [51264/60000]\n",
            "loss: 0.862387  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.857474 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.877419  [   64/60000]\n",
            "loss: 0.947290  [ 6464/60000]\n",
            "loss: 0.736236  [12864/60000]\n",
            "loss: 0.922878  [19264/60000]\n",
            "loss: 0.796230  [25664/60000]\n",
            "loss: 0.818988  [32064/60000]\n",
            "loss: 0.875538  [38464/60000]\n",
            "loss: 0.829036  [44864/60000]\n",
            "loss: 0.848093  [51264/60000]\n",
            "loss: 0.825907  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.818432 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.824943  [   64/60000]\n",
            "loss: 0.907920  [ 6464/60000]\n",
            "loss: 0.690122  [12864/60000]\n",
            "loss: 0.885528  [19264/60000]\n",
            "loss: 0.762853  [25664/60000]\n",
            "loss: 0.779530  [32064/60000]\n",
            "loss: 0.841597  [38464/60000]\n",
            "loss: 0.805386  [44864/60000]\n",
            "loss: 0.814219  [51264/60000]\n",
            "loss: 0.796364  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.787479 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.781594  [   64/60000]\n",
            "loss: 0.874662  [ 6464/60000]\n",
            "loss: 0.652649  [12864/60000]\n",
            "loss: 0.855842  [19264/60000]\n",
            "loss: 0.736068  [25664/60000]\n",
            "loss: 0.748074  [32064/60000]\n",
            "loss: 0.812849  [38464/60000]\n",
            "loss: 0.786680  [44864/60000]\n",
            "loss: 0.786754  [51264/60000]\n",
            "loss: 0.771520  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.761782 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.744662  [   64/60000]\n",
            "loss: 0.845281  [ 6464/60000]\n",
            "loss: 0.621250  [12864/60000]\n",
            "loss: 0.831263  [19264/60000]\n",
            "loss: 0.713647  [25664/60000]\n",
            "loss: 0.722524  [32064/60000]\n",
            "loss: 0.787643  [38464/60000]\n",
            "loss: 0.770889  [44864/60000]\n",
            "loss: 0.763749  [51264/60000]\n",
            "loss: 0.749766  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.739621 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.712372  [   64/60000]\n",
            "loss: 0.818575  [ 6464/60000]\n",
            "loss: 0.594228  [12864/60000]\n",
            "loss: 0.810468  [19264/60000]\n",
            "loss: 0.694562  [25664/60000]\n",
            "loss: 0.701220  [32064/60000]\n",
            "loss: 0.764838  [38464/60000]\n",
            "loss: 0.756866  [44864/60000]\n",
            "loss: 0.743986  [51264/60000]\n",
            "loss: 0.730270  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.719948 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.683493  [   64/60000]\n",
            "loss: 0.794047  [ 6464/60000]\n",
            "loss: 0.570585  [12864/60000]\n",
            "loss: 0.792321  [19264/60000]\n",
            "loss: 0.678087  [25664/60000]\n",
            "loss: 0.683042  [32064/60000]\n",
            "loss: 0.743792  [38464/60000]\n",
            "loss: 0.744036  [44864/60000]\n",
            "loss: 0.726529  [51264/60000]\n",
            "loss: 0.712551  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.702100 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.657506  [   64/60000]\n",
            "loss: 0.771273  [ 6464/60000]\n",
            "loss: 0.549551  [12864/60000]\n",
            "loss: 0.776125  [19264/60000]\n",
            "loss: 0.663584  [25664/60000]\n",
            "loss: 0.667330  [32064/60000]\n",
            "loss: 0.724178  [38464/60000]\n",
            "loss: 0.732070  [44864/60000]\n",
            "loss: 0.711025  [51264/60000]\n",
            "loss: 0.696063  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.685688 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.633970  [   64/60000]\n",
            "loss: 0.750100  [ 6464/60000]\n",
            "loss: 0.530691  [12864/60000]\n",
            "loss: 0.761379  [19264/60000]\n",
            "loss: 0.650692  [25664/60000]\n",
            "loss: 0.653625  [32064/60000]\n",
            "loss: 0.705796  [38464/60000]\n",
            "loss: 0.721062  [44864/60000]\n",
            "loss: 0.697088  [51264/60000]\n",
            "loss: 0.680588  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.670496 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.612577  [   64/60000]\n",
            "loss: 0.730396  [ 6464/60000]\n",
            "loss: 0.513518  [12864/60000]\n",
            "loss: 0.747851  [19264/60000]\n",
            "loss: 0.639275  [25664/60000]\n",
            "loss: 0.641539  [32064/60000]\n",
            "loss: 0.688590  [38464/60000]\n",
            "loss: 0.710901  [44864/60000]\n",
            "loss: 0.684585  [51264/60000]\n",
            "loss: 0.666102  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.656412 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.592995  [   64/60000]\n",
            "loss: 0.712016  [ 6464/60000]\n",
            "loss: 0.497929  [12864/60000]\n",
            "loss: 0.735276  [19264/60000]\n",
            "loss: 0.629085  [25664/60000]\n",
            "loss: 0.630773  [32064/60000]\n",
            "loss: 0.672401  [38464/60000]\n",
            "loss: 0.701720  [44864/60000]\n",
            "loss: 0.673623  [51264/60000]\n",
            "loss: 0.652521  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.643360 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.575077  [   64/60000]\n",
            "loss: 0.694958  [ 6464/60000]\n",
            "loss: 0.483746  [12864/60000]\n",
            "loss: 0.723555  [19264/60000]\n",
            "loss: 0.619958  [25664/60000]\n",
            "loss: 0.621167  [32064/60000]\n",
            "loss: 0.657321  [38464/60000]\n",
            "loss: 0.693475  [44864/60000]\n",
            "loss: 0.664022  [51264/60000]\n",
            "loss: 0.639742  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.631271 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.558710  [   64/60000]\n",
            "loss: 0.679109  [ 6464/60000]\n",
            "loss: 0.470817  [12864/60000]\n",
            "loss: 0.712552  [19264/60000]\n",
            "loss: 0.611715  [25664/60000]\n",
            "loss: 0.612540  [32064/60000]\n",
            "loss: 0.643241  [38464/60000]\n",
            "loss: 0.686197  [44864/60000]\n",
            "loss: 0.655581  [51264/60000]\n",
            "loss: 0.627657  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.620086 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.543697  [   64/60000]\n",
            "loss: 0.664362  [ 6464/60000]\n",
            "loss: 0.459027  [12864/60000]\n",
            "loss: 0.702139  [19264/60000]\n",
            "loss: 0.604212  [25664/60000]\n",
            "loss: 0.604787  [32064/60000]\n",
            "loss: 0.630153  [38464/60000]\n",
            "loss: 0.679912  [44864/60000]\n",
            "loss: 0.648242  [51264/60000]\n",
            "loss: 0.616240  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.609757 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.529909  [   64/60000]\n",
            "loss: 0.650681  [ 6464/60000]\n",
            "loss: 0.448196  [12864/60000]\n",
            "loss: 0.692255  [19264/60000]\n",
            "loss: 0.597306  [25664/60000]\n",
            "loss: 0.597711  [32064/60000]\n",
            "loss: 0.617997  [38464/60000]\n",
            "loss: 0.674589  [44864/60000]\n",
            "loss: 0.641938  [51264/60000]\n",
            "loss: 0.605376  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.3%, Avg loss: 0.600223 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.517217  [   64/60000]\n",
            "loss: 0.638033  [ 6464/60000]\n",
            "loss: 0.438257  [12864/60000]\n",
            "loss: 0.682854  [19264/60000]\n",
            "loss: 0.590784  [25664/60000]\n",
            "loss: 0.591145  [32064/60000]\n",
            "loss: 0.606759  [38464/60000]\n",
            "loss: 0.670159  [44864/60000]\n",
            "loss: 0.636533  [51264/60000]\n",
            "loss: 0.595012  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.591425 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.505459  [   64/60000]\n",
            "loss: 0.626311  [ 6464/60000]\n",
            "loss: 0.429040  [12864/60000]\n",
            "loss: 0.673987  [19264/60000]\n",
            "loss: 0.584573  [25664/60000]\n",
            "loss: 0.584956  [32064/60000]\n",
            "loss: 0.596353  [38464/60000]\n",
            "loss: 0.666553  [44864/60000]\n",
            "loss: 0.631873  [51264/60000]\n",
            "loss: 0.585054  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.8%, Avg loss: 0.583292 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.494532  [   64/60000]\n",
            "loss: 0.615436  [ 6464/60000]\n",
            "loss: 0.420477  [12864/60000]\n",
            "loss: 0.665552  [19264/60000]\n",
            "loss: 0.578599  [25664/60000]\n",
            "loss: 0.579099  [32064/60000]\n",
            "loss: 0.586770  [38464/60000]\n",
            "loss: 0.663755  [44864/60000]\n",
            "loss: 0.627776  [51264/60000]\n",
            "loss: 0.575514  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.575781 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.484364  [   64/60000]\n",
            "loss: 0.605362  [ 6464/60000]\n",
            "loss: 0.412579  [12864/60000]\n",
            "loss: 0.657632  [19264/60000]\n",
            "loss: 0.572830  [25664/60000]\n",
            "loss: 0.573545  [32064/60000]\n",
            "loss: 0.577949  [38464/60000]\n",
            "loss: 0.661687  [44864/60000]\n",
            "loss: 0.624327  [51264/60000]\n",
            "loss: 0.566408  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.568831 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.474925  [   64/60000]\n",
            "loss: 0.596117  [ 6464/60000]\n",
            "loss: 0.405203  [12864/60000]\n",
            "loss: 0.650129  [19264/60000]\n",
            "loss: 0.567123  [25664/60000]\n",
            "loss: 0.568214  [32064/60000]\n",
            "loss: 0.569706  [38464/60000]\n",
            "loss: 0.660210  [44864/60000]\n",
            "loss: 0.621315  [51264/60000]\n",
            "loss: 0.557556  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.562400 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.466096  [   64/60000]\n",
            "loss: 0.587528  [ 6464/60000]\n",
            "loss: 0.398335  [12864/60000]\n",
            "loss: 0.642925  [19264/60000]\n",
            "loss: 0.561513  [25664/60000]\n",
            "loss: 0.563063  [32064/60000]\n",
            "loss: 0.562159  [38464/60000]\n",
            "loss: 0.659311  [44864/60000]\n",
            "loss: 0.618707  [51264/60000]\n",
            "loss: 0.549011  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.556446 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.457908  [   64/60000]\n",
            "loss: 0.579592  [ 6464/60000]\n",
            "loss: 0.391958  [12864/60000]\n",
            "loss: 0.636031  [19264/60000]\n",
            "loss: 0.555978  [25664/60000]\n",
            "loss: 0.557933  [32064/60000]\n",
            "loss: 0.555142  [38464/60000]\n",
            "loss: 0.658853  [44864/60000]\n",
            "loss: 0.616347  [51264/60000]\n",
            "loss: 0.540823  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.550923 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.450230  [   64/60000]\n",
            "loss: 0.572244  [ 6464/60000]\n",
            "loss: 0.385992  [12864/60000]\n",
            "loss: 0.629400  [19264/60000]\n",
            "loss: 0.550548  [25664/60000]\n",
            "loss: 0.552917  [32064/60000]\n",
            "loss: 0.548608  [38464/60000]\n",
            "loss: 0.658768  [44864/60000]\n",
            "loss: 0.614182  [51264/60000]\n",
            "loss: 0.532915  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.545789 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.442979  [   64/60000]\n",
            "loss: 0.565479  [ 6464/60000]\n",
            "loss: 0.380389  [12864/60000]\n",
            "loss: 0.623066  [19264/60000]\n",
            "loss: 0.545149  [25664/60000]\n",
            "loss: 0.547910  [32064/60000]\n",
            "loss: 0.542574  [38464/60000]\n",
            "loss: 0.658974  [44864/60000]\n",
            "loss: 0.612214  [51264/60000]\n",
            "loss: 0.525329  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.541012 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.436098  [   64/60000]\n",
            "loss: 0.559248  [ 6464/60000]\n",
            "loss: 0.375098  [12864/60000]\n",
            "loss: 0.617024  [19264/60000]\n",
            "loss: 0.539811  [25664/60000]\n",
            "loss: 0.542907  [32064/60000]\n",
            "loss: 0.536984  [38464/60000]\n",
            "loss: 0.659383  [44864/60000]\n",
            "loss: 0.610427  [51264/60000]\n",
            "loss: 0.518040  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.536557 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.429568  [   64/60000]\n",
            "loss: 0.553474  [ 6464/60000]\n",
            "loss: 0.370139  [12864/60000]\n",
            "loss: 0.611266  [19264/60000]\n",
            "loss: 0.534627  [25664/60000]\n",
            "loss: 0.538005  [32064/60000]\n",
            "loss: 0.531770  [38464/60000]\n",
            "loss: 0.659924  [44864/60000]\n",
            "loss: 0.608732  [51264/60000]\n",
            "loss: 0.511082  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.532393 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.423374  [   64/60000]\n",
            "loss: 0.548108  [ 6464/60000]\n",
            "loss: 0.365498  [12864/60000]\n",
            "loss: 0.605730  [19264/60000]\n",
            "loss: 0.529514  [25664/60000]\n",
            "loss: 0.533169  [32064/60000]\n",
            "loss: 0.526957  [38464/60000]\n",
            "loss: 0.660545  [44864/60000]\n",
            "loss: 0.607148  [51264/60000]\n",
            "loss: 0.504410  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.528493 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.417459  [   64/60000]\n",
            "loss: 0.543155  [ 6464/60000]\n",
            "loss: 0.361108  [12864/60000]\n",
            "loss: 0.600403  [19264/60000]\n",
            "loss: 0.524516  [25664/60000]\n",
            "loss: 0.528387  [32064/60000]\n",
            "loss: 0.522448  [38464/60000]\n",
            "loss: 0.661140  [44864/60000]\n",
            "loss: 0.605620  [51264/60000]\n",
            "loss: 0.498075  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.524836 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.411829  [   64/60000]\n",
            "loss: 0.538553  [ 6464/60000]\n",
            "loss: 0.356941  [12864/60000]\n",
            "loss: 0.595316  [19264/60000]\n",
            "loss: 0.519648  [25664/60000]\n",
            "loss: 0.523712  [32064/60000]\n",
            "loss: 0.518204  [38464/60000]\n",
            "loss: 0.661640  [44864/60000]\n",
            "loss: 0.604119  [51264/60000]\n",
            "loss: 0.492043  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.521397 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.406521  [   64/60000]\n",
            "loss: 0.534251  [ 6464/60000]\n",
            "loss: 0.353018  [12864/60000]\n",
            "loss: 0.590462  [19264/60000]\n",
            "loss: 0.514895  [25664/60000]\n",
            "loss: 0.519145  [32064/60000]\n",
            "loss: 0.514192  [38464/60000]\n",
            "loss: 0.662075  [44864/60000]\n",
            "loss: 0.602668  [51264/60000]\n",
            "loss: 0.486332  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.518157 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.401442  [   64/60000]\n",
            "loss: 0.530243  [ 6464/60000]\n",
            "loss: 0.349335  [12864/60000]\n",
            "loss: 0.585802  [19264/60000]\n",
            "loss: 0.510274  [25664/60000]\n",
            "loss: 0.514681  [32064/60000]\n",
            "loss: 0.510391  [38464/60000]\n",
            "loss: 0.662396  [44864/60000]\n",
            "loss: 0.601263  [51264/60000]\n",
            "loss: 0.480936  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.515097 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.396564  [   64/60000]\n",
            "loss: 0.526484  [ 6464/60000]\n",
            "loss: 0.345868  [12864/60000]\n",
            "loss: 0.581311  [19264/60000]\n",
            "loss: 0.505805  [25664/60000]\n",
            "loss: 0.510316  [32064/60000]\n",
            "loss: 0.506766  [38464/60000]\n",
            "loss: 0.662597  [44864/60000]\n",
            "loss: 0.599869  [51264/60000]\n",
            "loss: 0.475798  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.512199 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.391853  [   64/60000]\n",
            "loss: 0.522994  [ 6464/60000]\n",
            "loss: 0.342573  [12864/60000]\n",
            "loss: 0.577012  [19264/60000]\n",
            "loss: 0.501461  [25664/60000]\n",
            "loss: 0.506136  [32064/60000]\n",
            "loss: 0.503330  [38464/60000]\n",
            "loss: 0.662681  [44864/60000]\n",
            "loss: 0.598469  [51264/60000]\n",
            "loss: 0.470920  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.509449 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.387348  [   64/60000]\n",
            "loss: 0.519724  [ 6464/60000]\n",
            "loss: 0.339446  [12864/60000]\n",
            "loss: 0.572867  [19264/60000]\n",
            "loss: 0.497273  [25664/60000]\n",
            "loss: 0.502064  [32064/60000]\n",
            "loss: 0.500074  [38464/60000]\n",
            "loss: 0.662627  [44864/60000]\n",
            "loss: 0.597130  [51264/60000]\n",
            "loss: 0.466265  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.506830 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.383038  [   64/60000]\n",
            "loss: 0.516674  [ 6464/60000]\n",
            "loss: 0.336467  [12864/60000]\n",
            "loss: 0.568928  [19264/60000]\n",
            "loss: 0.493166  [25664/60000]\n",
            "loss: 0.498083  [32064/60000]\n",
            "loss: 0.496996  [38464/60000]\n",
            "loss: 0.662409  [44864/60000]\n",
            "loss: 0.595768  [51264/60000]\n",
            "loss: 0.461848  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.504338 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.378891  [   64/60000]\n",
            "loss: 0.513773  [ 6464/60000]\n",
            "loss: 0.333615  [12864/60000]\n",
            "loss: 0.565143  [19264/60000]\n",
            "loss: 0.489210  [25664/60000]\n",
            "loss: 0.494270  [32064/60000]\n",
            "loss: 0.494056  [38464/60000]\n",
            "loss: 0.661948  [44864/60000]\n",
            "loss: 0.594442  [51264/60000]\n",
            "loss: 0.457673  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.501964 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.374858  [   64/60000]\n",
            "loss: 0.511043  [ 6464/60000]\n",
            "loss: 0.330893  [12864/60000]\n",
            "loss: 0.561537  [19264/60000]\n",
            "loss: 0.485369  [25664/60000]\n",
            "loss: 0.490624  [32064/60000]\n",
            "loss: 0.491224  [38464/60000]\n",
            "loss: 0.661346  [44864/60000]\n",
            "loss: 0.593092  [51264/60000]\n",
            "loss: 0.453714  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.499692 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.370955  [   64/60000]\n",
            "loss: 0.508441  [ 6464/60000]\n",
            "loss: 0.328296  [12864/60000]\n",
            "loss: 0.558086  [19264/60000]\n",
            "loss: 0.481543  [25664/60000]\n",
            "loss: 0.487116  [32064/60000]\n",
            "loss: 0.488498  [38464/60000]\n",
            "loss: 0.660633  [44864/60000]\n",
            "loss: 0.591756  [51264/60000]\n",
            "loss: 0.449994  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.497517 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.367205  [   64/60000]\n",
            "loss: 0.505938  [ 6464/60000]\n",
            "loss: 0.325812  [12864/60000]\n",
            "loss: 0.554780  [19264/60000]\n",
            "loss: 0.477838  [25664/60000]\n",
            "loss: 0.483711  [32064/60000]\n",
            "loss: 0.485872  [38464/60000]\n",
            "loss: 0.659727  [44864/60000]\n",
            "loss: 0.590374  [51264/60000]\n",
            "loss: 0.446496  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.495425 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.363623  [   64/60000]\n",
            "loss: 0.503533  [ 6464/60000]\n",
            "loss: 0.323406  [12864/60000]\n",
            "loss: 0.551587  [19264/60000]\n",
            "loss: 0.474214  [25664/60000]\n",
            "loss: 0.480486  [32064/60000]\n",
            "loss: 0.483336  [38464/60000]\n",
            "loss: 0.658728  [44864/60000]\n",
            "loss: 0.589015  [51264/60000]\n",
            "loss: 0.443121  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.493415 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.360086  [   64/60000]\n",
            "loss: 0.501232  [ 6464/60000]\n",
            "loss: 0.321068  [12864/60000]\n",
            "loss: 0.548555  [19264/60000]\n",
            "loss: 0.470713  [25664/60000]\n",
            "loss: 0.477413  [32064/60000]\n",
            "loss: 0.480941  [38464/60000]\n",
            "loss: 0.657608  [44864/60000]\n",
            "loss: 0.587635  [51264/60000]\n",
            "loss: 0.439948  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.491487 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.356684  [   64/60000]\n",
            "loss: 0.499061  [ 6464/60000]\n",
            "loss: 0.318783  [12864/60000]\n",
            "loss: 0.545685  [19264/60000]\n",
            "loss: 0.467345  [25664/60000]\n",
            "loss: 0.474429  [32064/60000]\n",
            "loss: 0.478653  [38464/60000]\n",
            "loss: 0.656370  [44864/60000]\n",
            "loss: 0.586267  [51264/60000]\n",
            "loss: 0.437001  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.489636 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.353383  [   64/60000]\n",
            "loss: 0.496935  [ 6464/60000]\n",
            "loss: 0.316577  [12864/60000]\n",
            "loss: 0.542925  [19264/60000]\n",
            "loss: 0.464071  [25664/60000]\n",
            "loss: 0.471554  [32064/60000]\n",
            "loss: 0.476432  [38464/60000]\n",
            "loss: 0.655187  [44864/60000]\n",
            "loss: 0.584952  [51264/60000]\n",
            "loss: 0.434185  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.487847 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.350135  [   64/60000]\n",
            "loss: 0.494921  [ 6464/60000]\n",
            "loss: 0.314440  [12864/60000]\n",
            "loss: 0.540265  [19264/60000]\n",
            "loss: 0.460888  [25664/60000]\n",
            "loss: 0.468837  [32064/60000]\n",
            "loss: 0.474297  [38464/60000]\n",
            "loss: 0.653922  [44864/60000]\n",
            "loss: 0.583673  [51264/60000]\n",
            "loss: 0.431510  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.486124 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.346991  [   64/60000]\n",
            "loss: 0.492944  [ 6464/60000]\n",
            "loss: 0.312407  [12864/60000]\n",
            "loss: 0.537698  [19264/60000]\n",
            "loss: 0.457773  [25664/60000]\n",
            "loss: 0.466311  [32064/60000]\n",
            "loss: 0.472258  [38464/60000]\n",
            "loss: 0.652555  [44864/60000]\n",
            "loss: 0.582400  [51264/60000]\n",
            "loss: 0.428992  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.484459 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.343900  [   64/60000]\n",
            "loss: 0.490980  [ 6464/60000]\n",
            "loss: 0.310435  [12864/60000]\n",
            "loss: 0.535190  [19264/60000]\n",
            "loss: 0.454772  [25664/60000]\n",
            "loss: 0.463875  [32064/60000]\n",
            "loss: 0.470271  [38464/60000]\n",
            "loss: 0.651132  [44864/60000]\n",
            "loss: 0.581104  [51264/60000]\n",
            "loss: 0.426559  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.482845 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.340856  [   64/60000]\n",
            "loss: 0.489088  [ 6464/60000]\n",
            "loss: 0.308516  [12864/60000]\n",
            "loss: 0.532786  [19264/60000]\n",
            "loss: 0.451819  [25664/60000]\n",
            "loss: 0.461570  [32064/60000]\n",
            "loss: 0.468315  [38464/60000]\n",
            "loss: 0.649710  [44864/60000]\n",
            "loss: 0.579831  [51264/60000]\n",
            "loss: 0.424230  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.481281 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.337924  [   64/60000]\n",
            "loss: 0.487245  [ 6464/60000]\n",
            "loss: 0.306649  [12864/60000]\n",
            "loss: 0.530503  [19264/60000]\n",
            "loss: 0.448869  [25664/60000]\n",
            "loss: 0.459410  [32064/60000]\n",
            "loss: 0.466419  [38464/60000]\n",
            "loss: 0.648258  [44864/60000]\n",
            "loss: 0.578616  [51264/60000]\n",
            "loss: 0.422032  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.479761 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.335111  [   64/60000]\n",
            "loss: 0.485425  [ 6464/60000]\n",
            "loss: 0.304806  [12864/60000]\n",
            "loss: 0.528289  [19264/60000]\n",
            "loss: 0.446116  [25664/60000]\n",
            "loss: 0.457332  [32064/60000]\n",
            "loss: 0.464634  [38464/60000]\n",
            "loss: 0.646666  [44864/60000]\n",
            "loss: 0.577402  [51264/60000]\n",
            "loss: 0.420004  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.478268 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.332389  [   64/60000]\n",
            "loss: 0.483606  [ 6464/60000]\n",
            "loss: 0.302972  [12864/60000]\n",
            "loss: 0.526183  [19264/60000]\n",
            "loss: 0.443518  [25664/60000]\n",
            "loss: 0.455301  [32064/60000]\n",
            "loss: 0.462896  [38464/60000]\n",
            "loss: 0.645044  [44864/60000]\n",
            "loss: 0.576009  [51264/60000]\n",
            "loss: 0.418168  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.476808 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.329780  [   64/60000]\n",
            "loss: 0.481822  [ 6464/60000]\n",
            "loss: 0.301269  [12864/60000]\n",
            "loss: 0.524170  [19264/60000]\n",
            "loss: 0.440900  [25664/60000]\n",
            "loss: 0.453280  [32064/60000]\n",
            "loss: 0.461193  [38464/60000]\n",
            "loss: 0.643332  [44864/60000]\n",
            "loss: 0.574670  [51264/60000]\n",
            "loss: 0.416426  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.475398 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.327282  [   64/60000]\n",
            "loss: 0.480129  [ 6464/60000]\n",
            "loss: 0.299667  [12864/60000]\n",
            "loss: 0.522195  [19264/60000]\n",
            "loss: 0.438328  [25664/60000]\n",
            "loss: 0.451312  [32064/60000]\n",
            "loss: 0.459452  [38464/60000]\n",
            "loss: 0.641515  [44864/60000]\n",
            "loss: 0.573201  [51264/60000]\n",
            "loss: 0.414774  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.474029 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.324851  [   64/60000]\n",
            "loss: 0.478439  [ 6464/60000]\n",
            "loss: 0.298079  [12864/60000]\n",
            "loss: 0.520214  [19264/60000]\n",
            "loss: 0.435720  [25664/60000]\n",
            "loss: 0.449384  [32064/60000]\n",
            "loss: 0.457691  [38464/60000]\n",
            "loss: 0.639768  [44864/60000]\n",
            "loss: 0.571654  [51264/60000]\n",
            "loss: 0.413187  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.472698 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.322524  [   64/60000]\n",
            "loss: 0.476806  [ 6464/60000]\n",
            "loss: 0.296484  [12864/60000]\n",
            "loss: 0.518307  [19264/60000]\n",
            "loss: 0.433216  [25664/60000]\n",
            "loss: 0.447469  [32064/60000]\n",
            "loss: 0.456065  [38464/60000]\n",
            "loss: 0.638028  [44864/60000]\n",
            "loss: 0.570143  [51264/60000]\n",
            "loss: 0.411618  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.471409 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.320227  [   64/60000]\n",
            "loss: 0.475191  [ 6464/60000]\n",
            "loss: 0.294957  [12864/60000]\n",
            "loss: 0.516382  [19264/60000]\n",
            "loss: 0.430762  [25664/60000]\n",
            "loss: 0.445627  [32064/60000]\n",
            "loss: 0.454513  [38464/60000]\n",
            "loss: 0.636382  [44864/60000]\n",
            "loss: 0.568781  [51264/60000]\n",
            "loss: 0.409995  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.470143 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.318015  [   64/60000]\n",
            "loss: 0.473590  [ 6464/60000]\n",
            "loss: 0.293427  [12864/60000]\n",
            "loss: 0.514532  [19264/60000]\n",
            "loss: 0.428270  [25664/60000]\n",
            "loss: 0.443910  [32064/60000]\n",
            "loss: 0.452995  [38464/60000]\n",
            "loss: 0.634819  [44864/60000]\n",
            "loss: 0.567524  [51264/60000]\n",
            "loss: 0.408495  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.468903 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.315815  [   64/60000]\n",
            "loss: 0.471996  [ 6464/60000]\n",
            "loss: 0.291941  [12864/60000]\n",
            "loss: 0.512747  [19264/60000]\n",
            "loss: 0.425814  [25664/60000]\n",
            "loss: 0.442241  [32064/60000]\n",
            "loss: 0.451554  [38464/60000]\n",
            "loss: 0.633274  [44864/60000]\n",
            "loss: 0.566293  [51264/60000]\n",
            "loss: 0.407024  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.467690 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.313650  [   64/60000]\n",
            "loss: 0.470441  [ 6464/60000]\n",
            "loss: 0.290556  [12864/60000]\n",
            "loss: 0.511035  [19264/60000]\n",
            "loss: 0.423485  [25664/60000]\n",
            "loss: 0.440680  [32064/60000]\n",
            "loss: 0.450160  [38464/60000]\n",
            "loss: 0.631747  [44864/60000]\n",
            "loss: 0.565093  [51264/60000]\n",
            "loss: 0.405664  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.466506 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.311564  [   64/60000]\n",
            "loss: 0.468904  [ 6464/60000]\n",
            "loss: 0.289188  [12864/60000]\n",
            "loss: 0.509325  [19264/60000]\n",
            "loss: 0.421183  [25664/60000]\n",
            "loss: 0.439170  [32064/60000]\n",
            "loss: 0.448790  [38464/60000]\n",
            "loss: 0.630162  [44864/60000]\n",
            "loss: 0.563880  [51264/60000]\n",
            "loss: 0.404373  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.465347 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.309533  [   64/60000]\n",
            "loss: 0.467421  [ 6464/60000]\n",
            "loss: 0.287895  [12864/60000]\n",
            "loss: 0.507706  [19264/60000]\n",
            "loss: 0.418976  [25664/60000]\n",
            "loss: 0.437701  [32064/60000]\n",
            "loss: 0.447432  [38464/60000]\n",
            "loss: 0.628545  [44864/60000]\n",
            "loss: 0.562663  [51264/60000]\n",
            "loss: 0.403065  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.464203 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.307538  [   64/60000]\n",
            "loss: 0.465918  [ 6464/60000]\n",
            "loss: 0.286577  [12864/60000]\n",
            "loss: 0.506137  [19264/60000]\n",
            "loss: 0.416743  [25664/60000]\n",
            "loss: 0.436257  [32064/60000]\n",
            "loss: 0.446134  [38464/60000]\n",
            "loss: 0.627004  [44864/60000]\n",
            "loss: 0.561544  [51264/60000]\n",
            "loss: 0.401799  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.463084 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.305636  [   64/60000]\n",
            "loss: 0.464519  [ 6464/60000]\n",
            "loss: 0.285269  [12864/60000]\n",
            "loss: 0.504629  [19264/60000]\n",
            "loss: 0.414536  [25664/60000]\n",
            "loss: 0.434957  [32064/60000]\n",
            "loss: 0.444837  [38464/60000]\n",
            "loss: 0.625397  [44864/60000]\n",
            "loss: 0.560429  [51264/60000]\n",
            "loss: 0.400639  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.461994 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.303797  [   64/60000]\n",
            "loss: 0.463087  [ 6464/60000]\n",
            "loss: 0.284013  [12864/60000]\n",
            "loss: 0.503134  [19264/60000]\n",
            "loss: 0.412385  [25664/60000]\n",
            "loss: 0.433665  [32064/60000]\n",
            "loss: 0.443624  [38464/60000]\n",
            "loss: 0.623799  [44864/60000]\n",
            "loss: 0.559322  [51264/60000]\n",
            "loss: 0.399534  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.460924 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.302052  [   64/60000]\n",
            "loss: 0.461641  [ 6464/60000]\n",
            "loss: 0.282799  [12864/60000]\n",
            "loss: 0.501630  [19264/60000]\n",
            "loss: 0.410177  [25664/60000]\n",
            "loss: 0.432279  [32064/60000]\n",
            "loss: 0.442360  [38464/60000]\n",
            "loss: 0.622282  [44864/60000]\n",
            "loss: 0.558143  [51264/60000]\n",
            "loss: 0.398516  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.459873 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.300308  [   64/60000]\n",
            "loss: 0.460176  [ 6464/60000]\n",
            "loss: 0.281644  [12864/60000]\n",
            "loss: 0.500090  [19264/60000]\n",
            "loss: 0.408022  [25664/60000]\n",
            "loss: 0.430951  [32064/60000]\n",
            "loss: 0.441138  [38464/60000]\n",
            "loss: 0.620775  [44864/60000]\n",
            "loss: 0.557003  [51264/60000]\n",
            "loss: 0.397515  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.458843 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.298603  [   64/60000]\n",
            "loss: 0.458761  [ 6464/60000]\n",
            "loss: 0.280493  [12864/60000]\n",
            "loss: 0.498586  [19264/60000]\n",
            "loss: 0.405902  [25664/60000]\n",
            "loss: 0.429644  [32064/60000]\n",
            "loss: 0.439915  [38464/60000]\n",
            "loss: 0.619291  [44864/60000]\n",
            "loss: 0.555793  [51264/60000]\n",
            "loss: 0.396530  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.457832 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.296955  [   64/60000]\n",
            "loss: 0.457313  [ 6464/60000]\n",
            "loss: 0.279349  [12864/60000]\n",
            "loss: 0.497116  [19264/60000]\n",
            "loss: 0.403799  [25664/60000]\n",
            "loss: 0.428360  [32064/60000]\n",
            "loss: 0.438749  [38464/60000]\n",
            "loss: 0.617794  [44864/60000]\n",
            "loss: 0.554597  [51264/60000]\n",
            "loss: 0.395553  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 0.456840 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.295367  [   64/60000]\n",
            "loss: 0.455905  [ 6464/60000]\n",
            "loss: 0.278234  [12864/60000]\n",
            "loss: 0.495647  [19264/60000]\n",
            "loss: 0.401750  [25664/60000]\n",
            "loss: 0.427131  [32064/60000]\n",
            "loss: 0.437573  [38464/60000]\n",
            "loss: 0.616348  [44864/60000]\n",
            "loss: 0.553414  [51264/60000]\n",
            "loss: 0.394648  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.455867 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.293841  [   64/60000]\n",
            "loss: 0.454512  [ 6464/60000]\n",
            "loss: 0.277148  [12864/60000]\n",
            "loss: 0.494250  [19264/60000]\n",
            "loss: 0.399733  [25664/60000]\n",
            "loss: 0.425973  [32064/60000]\n",
            "loss: 0.436407  [38464/60000]\n",
            "loss: 0.614937  [44864/60000]\n",
            "loss: 0.552230  [51264/60000]\n",
            "loss: 0.393771  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.454906 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.292347  [   64/60000]\n",
            "loss: 0.453126  [ 6464/60000]\n",
            "loss: 0.276060  [12864/60000]\n",
            "loss: 0.492856  [19264/60000]\n",
            "loss: 0.397783  [25664/60000]\n",
            "loss: 0.424816  [32064/60000]\n",
            "loss: 0.435282  [38464/60000]\n",
            "loss: 0.613535  [44864/60000]\n",
            "loss: 0.551055  [51264/60000]\n",
            "loss: 0.392892  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.453964 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.290917  [   64/60000]\n",
            "loss: 0.451737  [ 6464/60000]\n",
            "loss: 0.275007  [12864/60000]\n",
            "loss: 0.491516  [19264/60000]\n",
            "loss: 0.395903  [25664/60000]\n",
            "loss: 0.423658  [32064/60000]\n",
            "loss: 0.434170  [38464/60000]\n",
            "loss: 0.612043  [44864/60000]\n",
            "loss: 0.549973  [51264/60000]\n",
            "loss: 0.392058  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.453035 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.289554  [   64/60000]\n",
            "loss: 0.450393  [ 6464/60000]\n",
            "loss: 0.273992  [12864/60000]\n",
            "loss: 0.490226  [19264/60000]\n",
            "loss: 0.394033  [25664/60000]\n",
            "loss: 0.422454  [32064/60000]\n",
            "loss: 0.433072  [38464/60000]\n",
            "loss: 0.610548  [44864/60000]\n",
            "loss: 0.548883  [51264/60000]\n",
            "loss: 0.391286  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.452118 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.288226  [   64/60000]\n",
            "loss: 0.449102  [ 6464/60000]\n",
            "loss: 0.273013  [12864/60000]\n",
            "loss: 0.488964  [19264/60000]\n",
            "loss: 0.392204  [25664/60000]\n",
            "loss: 0.421243  [32064/60000]\n",
            "loss: 0.431938  [38464/60000]\n",
            "loss: 0.609077  [44864/60000]\n",
            "loss: 0.547792  [51264/60000]\n",
            "loss: 0.390507  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.451215 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.286892  [   64/60000]\n",
            "loss: 0.447803  [ 6464/60000]\n",
            "loss: 0.272064  [12864/60000]\n",
            "loss: 0.487713  [19264/60000]\n",
            "loss: 0.390327  [25664/60000]\n",
            "loss: 0.420004  [32064/60000]\n",
            "loss: 0.430831  [38464/60000]\n",
            "loss: 0.607618  [44864/60000]\n",
            "loss: 0.546724  [51264/60000]\n",
            "loss: 0.389776  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.450328 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.285581  [   64/60000]\n",
            "loss: 0.446478  [ 6464/60000]\n",
            "loss: 0.271107  [12864/60000]\n",
            "loss: 0.486477  [19264/60000]\n",
            "loss: 0.388490  [25664/60000]\n",
            "loss: 0.418784  [32064/60000]\n",
            "loss: 0.429746  [38464/60000]\n",
            "loss: 0.606211  [44864/60000]\n",
            "loss: 0.545596  [51264/60000]\n",
            "loss: 0.389063  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.449446 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.284306  [   64/60000]\n",
            "loss: 0.445165  [ 6464/60000]\n",
            "loss: 0.270168  [12864/60000]\n",
            "loss: 0.485214  [19264/60000]\n",
            "loss: 0.386714  [25664/60000]\n",
            "loss: 0.417573  [32064/60000]\n",
            "loss: 0.428678  [38464/60000]\n",
            "loss: 0.604831  [44864/60000]\n",
            "loss: 0.544532  [51264/60000]\n",
            "loss: 0.388360  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.448584 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.283084  [   64/60000]\n",
            "loss: 0.443887  [ 6464/60000]\n",
            "loss: 0.269268  [12864/60000]\n",
            "loss: 0.483975  [19264/60000]\n",
            "loss: 0.384986  [25664/60000]\n",
            "loss: 0.416401  [32064/60000]\n",
            "loss: 0.427629  [38464/60000]\n",
            "loss: 0.603418  [44864/60000]\n",
            "loss: 0.543491  [51264/60000]\n",
            "loss: 0.387701  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.447734 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.281886  [   64/60000]\n",
            "loss: 0.442613  [ 6464/60000]\n",
            "loss: 0.268399  [12864/60000]\n",
            "loss: 0.482760  [19264/60000]\n",
            "loss: 0.383243  [25664/60000]\n",
            "loss: 0.415136  [32064/60000]\n",
            "loss: 0.426565  [38464/60000]\n",
            "loss: 0.602035  [44864/60000]\n",
            "loss: 0.542459  [51264/60000]\n",
            "loss: 0.387058  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.446889 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.280721  [   64/60000]\n",
            "loss: 0.441318  [ 6464/60000]\n",
            "loss: 0.267603  [12864/60000]\n",
            "loss: 0.481503  [19264/60000]\n",
            "loss: 0.381527  [25664/60000]\n",
            "loss: 0.413903  [32064/60000]\n",
            "loss: 0.425522  [38464/60000]\n",
            "loss: 0.600743  [44864/60000]\n",
            "loss: 0.541405  [51264/60000]\n",
            "loss: 0.386449  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.446057 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.279565  [   64/60000]\n",
            "loss: 0.440079  [ 6464/60000]\n",
            "loss: 0.266824  [12864/60000]\n",
            "loss: 0.480250  [19264/60000]\n",
            "loss: 0.379832  [25664/60000]\n",
            "loss: 0.412751  [32064/60000]\n",
            "loss: 0.424538  [38464/60000]\n",
            "loss: 0.599458  [44864/60000]\n",
            "loss: 0.540251  [51264/60000]\n",
            "loss: 0.385795  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.445236 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.278469  [   64/60000]\n",
            "loss: 0.438840  [ 6464/60000]\n",
            "loss: 0.266078  [12864/60000]\n",
            "loss: 0.478987  [19264/60000]\n",
            "loss: 0.378183  [25664/60000]\n",
            "loss: 0.411583  [32064/60000]\n",
            "loss: 0.423562  [38464/60000]\n",
            "loss: 0.598205  [44864/60000]\n",
            "loss: 0.539205  [51264/60000]\n",
            "loss: 0.385206  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.444424 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.277417  [   64/60000]\n",
            "loss: 0.437609  [ 6464/60000]\n",
            "loss: 0.265355  [12864/60000]\n",
            "loss: 0.477804  [19264/60000]\n",
            "loss: 0.376579  [25664/60000]\n",
            "loss: 0.410450  [32064/60000]\n",
            "loss: 0.422493  [38464/60000]\n",
            "loss: 0.596936  [44864/60000]\n",
            "loss: 0.538112  [51264/60000]\n",
            "loss: 0.384576  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.443617 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.276453  [   64/60000]\n",
            "loss: 0.436396  [ 6464/60000]\n",
            "loss: 0.264665  [12864/60000]\n",
            "loss: 0.476645  [19264/60000]\n",
            "loss: 0.375004  [25664/60000]\n",
            "loss: 0.409404  [32064/60000]\n",
            "loss: 0.421423  [38464/60000]\n",
            "loss: 0.595657  [44864/60000]\n",
            "loss: 0.537027  [51264/60000]\n",
            "loss: 0.384006  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.442817 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.275463  [   64/60000]\n",
            "loss: 0.435192  [ 6464/60000]\n",
            "loss: 0.263921  [12864/60000]\n",
            "loss: 0.475485  [19264/60000]\n",
            "loss: 0.373427  [25664/60000]\n",
            "loss: 0.408354  [32064/60000]\n",
            "loss: 0.420341  [38464/60000]\n",
            "loss: 0.594420  [44864/60000]\n",
            "loss: 0.535943  [51264/60000]\n",
            "loss: 0.383379  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.442026 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.274461  [   64/60000]\n",
            "loss: 0.433957  [ 6464/60000]\n",
            "loss: 0.263197  [12864/60000]\n",
            "loss: 0.474355  [19264/60000]\n",
            "loss: 0.371844  [25664/60000]\n",
            "loss: 0.407322  [32064/60000]\n",
            "loss: 0.419349  [38464/60000]\n",
            "loss: 0.593179  [44864/60000]\n",
            "loss: 0.534906  [51264/60000]\n",
            "loss: 0.382753  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.441242 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.273515  [   64/60000]\n",
            "loss: 0.432732  [ 6464/60000]\n",
            "loss: 0.262483  [12864/60000]\n",
            "loss: 0.473201  [19264/60000]\n",
            "loss: 0.370343  [25664/60000]\n",
            "loss: 0.406363  [32064/60000]\n",
            "loss: 0.418343  [38464/60000]\n",
            "loss: 0.591946  [44864/60000]\n",
            "loss: 0.533875  [51264/60000]\n",
            "loss: 0.382171  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.440471 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.272618  [   64/60000]\n",
            "loss: 0.431498  [ 6464/60000]\n",
            "loss: 0.261775  [12864/60000]\n",
            "loss: 0.471978  [19264/60000]\n",
            "loss: 0.368904  [25664/60000]\n",
            "loss: 0.405487  [32064/60000]\n",
            "loss: 0.417292  [38464/60000]\n",
            "loss: 0.590726  [44864/60000]\n",
            "loss: 0.532848  [51264/60000]\n",
            "loss: 0.381631  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.439706 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.271716  [   64/60000]\n",
            "loss: 0.430290  [ 6464/60000]\n",
            "loss: 0.261056  [12864/60000]\n",
            "loss: 0.470785  [19264/60000]\n",
            "loss: 0.367463  [25664/60000]\n",
            "loss: 0.404505  [32064/60000]\n",
            "loss: 0.416247  [38464/60000]\n",
            "loss: 0.589558  [44864/60000]\n",
            "loss: 0.531834  [51264/60000]\n",
            "loss: 0.381063  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.438953 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.270932  [   64/60000]\n",
            "loss: 0.429087  [ 6464/60000]\n",
            "loss: 0.260416  [12864/60000]\n",
            "loss: 0.469639  [19264/60000]\n",
            "loss: 0.366062  [25664/60000]\n",
            "loss: 0.403509  [32064/60000]\n",
            "loss: 0.415128  [38464/60000]\n",
            "loss: 0.588407  [44864/60000]\n",
            "loss: 0.530853  [51264/60000]\n",
            "loss: 0.380573  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.438196 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.270119  [   64/60000]\n",
            "loss: 0.427925  [ 6464/60000]\n",
            "loss: 0.259744  [12864/60000]\n",
            "loss: 0.468536  [19264/60000]\n",
            "loss: 0.364684  [25664/60000]\n",
            "loss: 0.402500  [32064/60000]\n",
            "loss: 0.413919  [38464/60000]\n",
            "loss: 0.587279  [44864/60000]\n",
            "loss: 0.529842  [51264/60000]\n",
            "loss: 0.380097  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.437447 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.269332  [   64/60000]\n",
            "loss: 0.426779  [ 6464/60000]\n",
            "loss: 0.259053  [12864/60000]\n",
            "loss: 0.467430  [19264/60000]\n",
            "loss: 0.363254  [25664/60000]\n",
            "loss: 0.401409  [32064/60000]\n",
            "loss: 0.412772  [38464/60000]\n",
            "loss: 0.586136  [44864/60000]\n",
            "loss: 0.528911  [51264/60000]\n",
            "loss: 0.379598  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.436714 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.268517  [   64/60000]\n",
            "loss: 0.425662  [ 6464/60000]\n",
            "loss: 0.258384  [12864/60000]\n",
            "loss: 0.466312  [19264/60000]\n",
            "loss: 0.361856  [25664/60000]\n",
            "loss: 0.400337  [32064/60000]\n",
            "loss: 0.411649  [38464/60000]\n",
            "loss: 0.584950  [44864/60000]\n",
            "loss: 0.527987  [51264/60000]\n",
            "loss: 0.379129  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.435996 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.267743  [   64/60000]\n",
            "loss: 0.424577  [ 6464/60000]\n",
            "loss: 0.257750  [12864/60000]\n",
            "loss: 0.465195  [19264/60000]\n",
            "loss: 0.360454  [25664/60000]\n",
            "loss: 0.399250  [32064/60000]\n",
            "loss: 0.410588  [38464/60000]\n",
            "loss: 0.583728  [44864/60000]\n",
            "loss: 0.526997  [51264/60000]\n",
            "loss: 0.378651  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 0.435282 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}